{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MCP with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install -q langgraph langchain-mcp-adapters langchain_google_genai pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import asyncio\n",
    "\n",
    "import os\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"ENTER_API_KEY_HERE\"\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"finance_mcp_server.py\"]\n",
    ")\n",
    "\n",
    "async def run_agent(message):\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools=await load_mcp_tools(session)\n",
    "            agent = create_react_agent(llm, tools)\n",
    "            agent_response = await agent.ainvoke(\n",
    "                {\"messages\": message}\n",
    "            )\n",
    "            return agent_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def display_messages(result):\n",
    "    \"\"\"Display formatted messages\"\"\"\n",
    "    for message in result[\"messages\"]:\n",
    "        display(Markdown(message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What's the revenue in 2024 from financial income statement for ticker TSLA\"\n",
    "result = asyncio.run(run_agent(question))\n",
    "display_messages(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " display(Markdown(result[\"messages\"][3].content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using multiple MCP Server in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"finance\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"finance_mcp_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"filesystem_mcp_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    agent = create_react_agent(llm, client.get_tools())\n",
    "    question = \"Get financial income statement for ticker TSLA and write to a file called tsla.json, to verfify read the file and display\"\n",
    "    response = await agent.ainvoke({\"messages\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_messages(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
